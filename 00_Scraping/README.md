# README

## About
This folder is dedicated to data collection, cleaning, and matching.

## Files and Folders

### Folders
- econ_lit_xml
    - This folder contains the .xml files that are generated by the EconLit batch citation tool. After an EconLit query is generated, the batch citation tool asks you to submit an email address. Some minutes later an email is sent to that address which provides a link (live for 24 hours after sending) that allows you to download this .xml. 
    - Submit these queries and download these downloads at the journal-level.
    - The format for the .xml filename should be "[PUBCODE]_[Start YY]-[End YY].xml"
    - These are raw data files; they should **NOT** be edited.
- econlit_scopus_matching_out
    - This folder contains the .csv files that underlie the papers dataset(s). There are 4 types of .csv files:
    1. '[PUBCODE]_author_abstract_funding.csv'
        - This .csv contains article-author-level observations from the [Scopus Abstract Retrieval API](https://dev.elsevier.com/documentation/AbstractRetrievalAPI.wadl). Each article-level observation contains doi, volume, issue, date, publication name, title, page-range,  
        - This .csv file is generated by `scopus_read.ipynb` in the `abstract_references_collect()` function.
        - These are effectively raw data files; they should **NOT** be edited. 
    2. '[PUBCODE]_cites.csv'
        - This .csv file contains article-citation-level observations from the [Scopus Abstract Retrieval API](). Each article-citation-level contains xxxxxxxxxxxxxxxxxxxxxxxxxx.
        - This .csv file is generated by `scopus_read.ipynb` in the `abstract_references_collect()` function.
        - These are effectively raw data files; the should **NOT** be edited.
    3. '[PUBCODE]_scopus_core.csv'
        - This .csv contains article-level observations from the [Scopus Search API](https://dev.elsevier.com/documentation/SCOPUSSearchAPI.wadl). Each article-level observation contains doi, title, publication name, volume, issue, page-range, date, and open-access status information. Additionally, columns for the article's corresponding abstract retrieval API endpoint and the API endpoint that was used to generate the observation of interest are made available.
        - This .csv file is generated by `scopus_read.ipynb` in the `publication_collect()` function.
        - These are raw data files; they should **NOT** be edited.
    4. '[pubcode]_econlit.csv' 
        - This .csv file contains article-level observations from EconLit. It is generated by `jel_scopus_matching.ipynb` which reads in the .xml files in the folder "00_Scraping/econ_lit_xml/". The article-level observations contains volume, issue, date, author, abstract, JEL code/description, title and some other extraneous data.
- nber_working_papers

### Scripts


### Misc.
- `aea_jel_codes.xml`
  - This is effectively a table of JEL codes and their descriptions exactly the way the AEA writes them. We use this for matching codes to descriptions. This file has been collected from the AEA's [website](https://www.aeaweb.org/econlit/jelCodes.php).
    - The codes and descriptions can be accessed in Python scripts using the following code
    ```python
        df = pd.read_xml('aea_jel_codes.xml', xpath='classification)
    ```
- `env_keys.json`
  - This contains API keys for SCOPUS. VERY IMPORTANT TO NOT UPLOAD THIS TO GITHUB. ENSURE THAT IT IS IN YOUR `.gitignore` FILE.
- `proxies.txt`
  - John is subscribed to a proxy-hosting service. We use these proxies to route our traffic when web-scraping so that our IPs are masked. Note that the proxy addresses in this .txt file are scrambled. To actually use them, split each line on colons (':') and then rearrange in order [2,3,0,1]. VERY IMPORTANT TO NOT UPLOAD THIS TO GITHUB. ENSURE THAT IT IS IN YOUR `.gitignore` FILE.
- `test.json`
  - There are a bunch of API calls that get made throughout many of these files. I usually format the response as JSON objects. I often copy-paste those objects into a separate .json file so that there is nice formatting when I am writing code/figuring out how to extract the actual data of interest. This is that .json file 
- `authors_of_interest.md`
  - We are briefly interested in looking at how (well) published certain IO, finance, and labor economists are. This is just a list of names with their Scopus author IDs.
