{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREAMBLE: LOADING NECESSARY PYTHON PACKAGES\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREAMBLE: LOADING LOCAL ENVIRONMENT VARIABLES (API KEYS) Accessing secure API keys\n",
    "keys_json = json.load(open(\"env_keys.json\"))\n",
    "scopus_key = keys_json['scopus_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SCOPUS API REQUEST HEADERS\n",
    "req_headers = {\n",
    "    'X-ELS-APIKey' : scopus_key\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a function that helps to fill out article-level observations from the API response without throwing too many failures or blowing up the whole process without getting at least the most important information. It is also used to collect as much author/abstract/reference information as possible without killing the whole process.\n",
    "def field_population(temp_df, json_obj, navigation_dict, field, optional_dict_modification=0):\n",
    "    # This is the json object that we want to explore. It could be an article- or an author-observation, for example.\n",
    "    json_object = json_obj\n",
    "    # print(json_object)\n",
    "    if field == list(navigation_dict.keys())[0]:\n",
    "        row_to_enter_info = len(temp_df)\n",
    "    else:\n",
    "        row_to_enter_info = len(temp_df)-1\n",
    "\n",
    "    # If a single affiliation has multiple authors associated with it, the structure that we used before has to be inverted. This is a cludgey way to get around that problem\n",
    "    if optional_dict_modification != 0:\n",
    "        if re.search(r'\\[\\'author\\'\\]\\[0\\]', navigation_dict[field], re.I):\n",
    "            # Change navigation pattern\n",
    "            original_navigation_string = '[\\'author\\'][0]'\n",
    "            modified_navigation_string = '[\\'author\\'][{}]'.format(optional_dict_modification)\n",
    "            modified_navigation_pattern = r'\\[\\'author\\'\\]\\[{}\\]'.format(optional_dict_modification)\n",
    "            navigation_dict[field] = re.sub(r'\\[\\'author\\'\\]\\[0\\]', modified_navigation_string, navigation_dict[field])\n",
    "\n",
    "            exec('temp_df.loc[{}, \"{}\"] = json_object{}'.format(row_to_enter_info, field, navigation_dict[field]))\n",
    "            #reset navigation pattern\n",
    "            # navigation_dict[field] = re.sub(modified_navigation_pattern, original_navigation_string, navigation_dict[field])\n",
    "\n",
    "    try:\n",
    "        ## We have only a small number of special cases so that the general function holds as much as possible:\n",
    "        if field == 'sc_author_affil_id':\n",
    "            # SOMETIMES AN AUTHOR HAS MULTIPLE AFFILIATIONS (EG NBER AND HARVARD). IN THIS CASE, WE CONCATENATE ALL OF THE AFFILIATIONS, DELIMITED BY '+' SYMBOLS AND STORE THEM AS A STRING (TO BE UNNESTED LATER)\n",
    "            affiliation_id_list_of_dicts  = json_object['affiliation']['affiliation-id']\n",
    "            if type(affiliation_id_list_of_dicts) == list:\n",
    "                multi_affil_list = []\n",
    "                for affil in affiliation_id_list_of_dicts:\n",
    "                    multi_affil_list.append(affil['@afid'])\n",
    "                sc_author_affil_id = '+'.join(multi_affil_list)\n",
    "                temp_df.loc[optional_dict_modification, field] = sc_author_affil_id\n",
    "            else:\n",
    "                exec('temp_df.loc[{}, \"{}\"] = json_object{}'.format(row_to_enter_info, field, navigation_dict[field]))\n",
    "        elif field == 'sc_funding_agency':\n",
    "            ## SOMETIMES THERE ARE MULTIPLE FUNDING AGENCIES. IN THIS CASE, WE CONCATENATE ALL OF THOSE AGENCIES, DELIMITED BY '+' SYMBOLS AND STORE THEM AS A STRING. I THINK THIS IS OKAY BECAUSE IF THERE IS AN IDENTIFIED FUNDING AGENCY, SCOPUS HAS ALSO COLLECTED (AT LEAST PART OF) THE 'THANKS' FOOTNOTE THAT DISCLOSES IT\n",
    "            funding_agencies_list_of_dicts  = json_object['abstracts-retrieval-response']['item']['xocs:meta']['xocs:funding']\n",
    "            if type(json_object) == list:\n",
    "                multi_agency_list = []\n",
    "                for agency in multi_agency_list:\n",
    "                    multi_agency_list.append(agency['xocs:funding-agency'])\n",
    "                sc_funding_agency = '+'.join(multi_agency_list)\n",
    "                temp_df.loc[optional_dict_modification, field] = sc_funding_agency\n",
    "            else:\n",
    "                exec('temp_df.loc[{}, \"{}\"] = json_object{}'.format(optional_dict_modification, field, navigation_dict[field]))\n",
    "        else: \n",
    "            exec('temp_df.loc[{}, \"{}\"] = json_object{}'.format(optional_dict_modification, field, navigation_dict[field]))\n",
    "    \n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        # print(\"SCOPUS FAILURE ON: {}\".format(field))\n",
    "        # print(e)\n",
    "        # print(traceback.format_exc())\n",
    "        # print('------------------------------------------------------')\n",
    "        temp_df.loc[optional_dict_modification, field] = 'SCOPUS FAILURE'\n",
    "\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an issn and a year in which to begin, this function collects all articles from the SCOPUS Search API and returns a publication-specific df \n",
    "def publication_collect(issn:str, start_year:str, journal_name:str):\n",
    "    # INSTANTIATE AN EMPTY DATAFRAME THAT WILL CONTAIN ALL OF THE RESULTS FOR THIS PUBLICATION\n",
    "    pub_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "    #### FIRST WE NEED TO SEE HOW MANY ARTICLES WE NEED TO COLLECT FROM THIS PUBLICATION\n",
    "    # How to construct a scopus query\n",
    "    # Guide: https://dev.elsevier.com/sc_search_tips.html\n",
    "    # Practice at: https://www.scopus.com/search/form.uri?display=advanced\n",
    "\n",
    "\n",
    "    current_year = 2021\n",
    "    human_query_issn = 'ISSN({issn_str})'.format(issn_str=issn)\n",
    "    human_query_date = '{start_year_str}-{current_year_str}'.format(start_year_str=start_year,\n",
    "                                                                    current_year_str=current_year)    \n",
    "    req_headers = {\n",
    "        'X-ELS-APIKey' : scopus_key\n",
    "    }\n",
    "\n",
    "    prelim_query = {\n",
    "        'httpAccept' : 'application/json',\n",
    "        'query' : human_query_issn,\n",
    "        'date' : human_query_date,\n",
    "        'count' : '1',\n",
    "        'cursor' : '*'\n",
    "    }\n",
    "\n",
    "    prelim_r = requests.get('https://api.elsevier.com/content/search/scopus',\n",
    "                            headers=req_headers,\n",
    "                            params=prelim_query)\n",
    "    print(\"PRELIM {pub_name} API STATUS CODE: {status}\".format(pub_name=journal_name, status=prelim_r.status_code))\n",
    "    prelim_json = prelim_r.json()\n",
    "    # print(prelim_json)\n",
    "    article_count = int(prelim_json['search-results']['opensearch:totalResults'])\n",
    "    print(\"PRELIM COUNT OF ARTICLES FOUND BY API: {}\".format(article_count))\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    \n",
    "    \n",
    "\n",
    "    still_more_pages_to_call = True\n",
    "\n",
    "    nth_call_counter = 1\n",
    "    ### THE API WILL ONLY RETURN 200 ARTICLES AT TIME, SO WE NEED TO CREATE A WHILE-LOOP THAT PULLS 200 RESULTS AT A TIME AS LONG AS THE CURRENT RESPONSE LINKS TO A POTENTIAL 'next' PAGE.\n",
    "    while still_more_pages_to_call:     \n",
    "        call_df = pd.DataFrame()\n",
    "        if nth_call_counter ==1: \n",
    "            print(\"\\t Preparing to make call {} to {}\".format(nth_call_counter, journal_name))\n",
    "            nth_call_counter += 1\n",
    "            call_query = {\n",
    "                'httpAccept' : 'application/json',\n",
    "                'query' : human_query_issn,\n",
    "                'date' : human_query_date,\n",
    "                'count' : '200',\n",
    "                'cursor' : '*' \n",
    "            }\n",
    "        else:\n",
    "            print(\"\\t Preparing to make call {} to {}\".format(nth_call_counter, journal_name))\n",
    "            nth_call_counter += 1\n",
    "            cursor_next_hash = r_json['cursor']['@next']\n",
    "\n",
    "            call_query = {\n",
    "                'httpAccept' : 'application/json',\n",
    "                'query' : human_query_issn,\n",
    "                'date' : human_query_date,\n",
    "                'count' : '200',\n",
    "                'cursor' : cursor_next_hash\n",
    "            }\n",
    "        \n",
    "\n",
    "        # print(call_query)\n",
    "\n",
    "        # THIS IS WHERE WE CALL THE API TO GET THE LIST OF ARTICLES (200 AT A TIME BY USING 'cursor/@next')\n",
    "        r = requests.get('https://api.elsevier.com/content/search/scopus',\n",
    "                        headers=req_headers,\n",
    "                        params=call_query)\n",
    "        \n",
    "        print(\"\\t CALL {n} FOR {pub_name} RETURNING STATUS CODE: {code}\".format(pub_name=journal_name,\n",
    "                                                                                    n=nth_call_counter - 1,\n",
    "                                                                                    code=r.status_code))\n",
    "        \n",
    "        \n",
    "        r_json = r.json()['search-results']\n",
    "\n",
    "        if r_json['cursor']['@current'] == r_json['cursor']['@next']:\n",
    "            print(\"---------------------------------------------\")\n",
    "            print(\"We have reached the end of the 'cursor-next' chain. breaking out of this pub\")\n",
    "            still_more_pages_to_call = False\n",
    "            print(\"---------------------------------------------\")\n",
    "            print(\"---------------------------------------------\")\n",
    "            print(\"---------------------------------------------\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Going to finish collecting this page and then there still at least one more to go.\")\n",
    "\n",
    "        # NOW WE BEGIN UNPACKING EACH BATCH OF 200 TO STORE IN A TEMP DF\n",
    "        sc_query_used = r_json['link'][0]['@href']\n",
    "        r_results = r_json['entry']\n",
    "        print(\"{pub_name} CALL {n} ARTICLES FOUND RESULTS: {num}\".format(pub_name=journal_name,\n",
    "                                                                                    n=nth_call_counter,\n",
    "                                                                                    num=len(r_results)))\n",
    "        \n",
    "        ## THIS DICTIONARY SHOULD BE STRUCTURED IN ORDER OF IMPORTANCE. IE. PULLING THE API ENDPOINT IS MORE IMPORTANT THAN PULLING THE DOI, WHICH IS MORE IMPORTANT THE TITLE ETC.\n",
    "        article_obj_nav_dict = {\n",
    "            'sc_abstract_api_endpoint' : '[\"link\"][0][\"@href\"]',\n",
    "            'doi' : \"['prism:doi']\",\n",
    "            'sc_title' : \"['dc:title']\",\n",
    "            'sc_issn' : \"['prism:issn']\",\n",
    "            'sc_pub_name' : \"['prism:publicationName']\",\n",
    "            'sc_pub_date' : \"['prism:coverDate']\",\n",
    "            'sc_open_access_status' : \"['openaccessFlag']\",\n",
    "            'sc_vol' : \"['prism:volume']\",\n",
    "            'sc_issue' : \"['prism:issueIdentifier']\",\n",
    "            'sc_page_range' : \"['prism:pageRange']\",\n",
    "            'sc_human_url' : \"['link'][3]['@href']\"\n",
    "        }\n",
    "\n",
    "        for article_obj in r_results:\n",
    "            # CONSTRUCT A TEMP_DF THAT CONTAINS A SINGLE ARTICLE THAT WILL BE APPENDED TO call_df\n",
    "            temp_df = pd.DataFrame({\n",
    "                'doi' : [None],\n",
    "                'sc_title' : [None],\n",
    "                'sc_issn' : [None],\n",
    "                'sc_pub_name' : [None],\n",
    "                'sc_vol' : [None],\n",
    "                'sc_issue' : [None],\n",
    "                'sc_page_range' : [None],\n",
    "                'sc_abstract_api_endpoint' : [None],\n",
    "                'sc_human_url' : [None]\n",
    "            })\n",
    "            for field in article_obj_nav_dict:\n",
    "                # FOR EACH OF THE FIELDS IDENTIFIED ABOVE (article_obj_nav_dict), EXECUTE THE field population FUNCTION THAT WILL TRY TO ACCESS THAT FIELD ACCORDING TO THE GIVEN SUBSCRIPT. IF UNAVAILABLE, CONTINUE WITH NOTATION OF FAILURE\n",
    "                temp_df = field_population(temp_df, article_obj, article_obj_nav_dict, field)\n",
    "\n",
    "            # ADD THE ARTICLE TO call_df\n",
    "            # print(temp_df)\n",
    "            call_df = pd.concat([call_df, temp_df], ignore_index=True)\n",
    "            \n",
    "\n",
    "        #ADD TO THE call_df THE CONSTRUCTED API-ENDPOINT QUERY THAT WAS USED TO GENERATE ALL OF THE RESULTS (ARTICLES) FOR THIS call_df\n",
    "        call_df['sc_query_used'] = sc_query_used\n",
    "        print(\"\\t Number of articles collected on this call : {}\".format(len(call_df)))\n",
    "\n",
    "        # ADD THE 200-BATCH OF ARTICLES TO pub_df\n",
    "        pub_df = pd.concat([pub_df, call_df], ignore_index=True)\n",
    "        print(\"\\t Number of articles cumulative collected for {}: {}\".format(journal_name, len(pub_df)))\n",
    "\n",
    "        ### TIMER HERE TO ENSURE WE DON'T EXCEED THE SCOPUS API'S QUERY THROTTLE\n",
    "        time.sleep(0.15)\n",
    "        print(\"---------------------------------------------\")\n",
    "\n",
    "\n",
    "    return pub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_dict = {\n",
    "    # 'AER' : {\n",
    "    #     'issn' : '00028282',\n",
    "    #     'start_year' : '1990',\n",
    "    #     'print_name' : 'American Economic Review'\n",
    "    # },\n",
    "    'ECA' : {\n",
    "        'issn' : '00129682',\n",
    "        'start_year' : '1990',\n",
    "        'print_name' : 'Econometrica'\n",
    "    },\n",
    "    'JPE' : {\n",
    "        'issn' : '00223808',\n",
    "        'start_year' : '1990',\n",
    "        'print_name' : 'Journal of Political Economy'\n",
    "    },\n",
    "    'QJE' : {\n",
    "        'issn' : '00335533',\n",
    "        'start_year' : '1990',\n",
    "        'print_name' : 'Quarterly Journal of Economics'\n",
    "    }, \n",
    "    'RES' : {\n",
    "        'issn' : '00346527',\n",
    "        'start_year' : '1990',\n",
    "        'print_name' : 'Review of Economic Studies' \n",
    "    },\n",
    "    'RJE' : {\n",
    "        'issn' : '07416261',\n",
    "        'start_year' : '1990',\n",
    "        'print_name' : 'RAND Journal of Economics'\n",
    "    },\n",
    "    'JOF' : {\n",
    "        'issn' : '00221082',\n",
    "        'start_year' : '1990',\n",
    "        'print_name' : 'Journal of Finance' \n",
    "    },\n",
    "    # 'JFE' : {\n",
    "    #     'issn' : '0304405X',\n",
    "    #     'start_year' : '1990',\n",
    "    #     'print_name' : 'Journal of Financial Economics' \n",
    "    # },\n",
    "    # 'RFS' : {\n",
    "    #     'issn' : '08939454',\n",
    "    #     'start_year' : '1990',\n",
    "    #     'print_name' : 'Review of Financial Studies' \n",
    "    # },\n",
    "    # 'JEM' : {\n",
    "    #     'issn' : '15309134',\n",
    "    #     'start_year' : '1990',\n",
    "    #     'print_name' : 'Journal of Economics and Management Strategy' \n",
    "    # },\n",
    "    # 'ALJ' : {\n",
    "    #     'issn' : '00036056',\n",
    "    #     'start_year' : '1990',\n",
    "    #     'print_name' : 'Antitrust Law Journal' \n",
    "    # },\n",
    "    # 'JLE' : {\n",
    "    #     'issn' : '00222186',\n",
    "    #     'start_year' : '1990',\n",
    "    #     'print_name' : 'Journal of Law and Economics' \n",
    "    # },\n",
    "    # 'JLO' : {\n",
    "    #     'issn' : '87566222',\n",
    "    #     'start_year' : '1990',\n",
    "    #     'print_name' : 'Journal of Law, Economics, and Organization' \n",
    "    # },\n",
    "    # 'JOL' : {\n",
    "    #     'issn' : '0734306X',\n",
    "    #     'start_year' : '1990',\n",
    "    #     'print_name' : 'Journal of Labor Economics' \n",
    "    # },\n",
    "    # 'JHR' : {\n",
    "    #     'issn' : '0022166X',\n",
    "    #     'start_year' : '1990',\n",
    "    #     'print_name' : 'Journal of Human Resources' \n",
    "    # },\n",
    "    # 'ATB' : {\n",
    "    #     'issn' : '0003603X',\n",
    "    #     'start_year' : '1990',\n",
    "    #     'print_name' : 'Antitrust Bulletin'\n",
    "    # }\n",
    "\n",
    "}\n",
    "# This is the dictionary that we are going to use to store and access the publication-specific dfs. We will programtically generate it based on the collection order (see above dict)\n",
    "pub_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MASTER RUN BLOCK HERE\n",
    "\n",
    "for pub in collection_dict.keys():\n",
    "    pub_dict[pub] = {}\n",
    "    pub_dict[pub]['{}_core_df'.format(pub)] = None\n",
    "    pub_dict[pub]['{}_author_abstract_df'.format(pub)] = None\n",
    "    pub_dict[pub]['{}_cites_df'.format(pub)] = None\n",
    "\n",
    "\n",
    "\n",
    "for pub in collection_dict.keys():\n",
    "\n",
    "\n",
    "    ##### 1. FIND ALL OF THE ARTICLES PUBLISHED IN THE JOURNALS OF INTEREST\n",
    "\n",
    "    pub_code = pub\n",
    "    pub_issn = collection_dict.get(pub).get('issn')\n",
    "    pub_start_year = collection_dict.get(pub).get('start_year')\n",
    "    pub_name = collection_dict.get(pub).get('print_name')\n",
    "\n",
    "    ## These are the lines of code that actually do things\n",
    "    # exec('{}_core_df = publication_collect(\"{}\", \"{}\",\"{}\")'.format(pub_code, pub_issn, pub_start_year, pub_name))\n",
    "    # exec('pub_dict[\"{}\"][\"{}_core_df\"] = {}_core_df'.format(pub_code,pub_code, pub_code))\n",
    "    # exec('{}_core_df.to_csv(\"scopus_data/{}_scopus_core.csv\",index=False, encoding=\"utf-8\")'.format(pub_code, pub_code))\n",
    "\n",
    "\n",
    "    ### In the case that the core_dfs have already been downloaded turn this chunk on and turn chunk 1 off.\n",
    "    print(pub_name)\n",
    "    pub_dict[pub_code]['{}_core_df'.format(pub_code)] = pd.read_csv('scopus_data/{}_scopus_core.csv'.format(pub_code))\n",
    "    print(len(pub_dict[pub_code]['{}_core_df'.format(pub_code)]))\n",
    "    print('-------------------------------------------------')\n",
    "\n",
    "\n",
    "    ##### 2. FIND THOSE ARTICLES' A) AUTHORS, ABSTRACTS, AND FUNDERS; AND B) PRIOR ARTICLES THAT THEY CITED\n",
    "    exec('{}_author_abstract_funding_df, {}_cites_df = abstract_references_collect(\"{}\")'.format(pub_code, pub_code, pub_code))\n",
    "    exec('pub_dict[\"{}\"][\"{}_author_abstract_df\"] = {}_author_abstract_funding_df'.format(pub_code,pub_code, pub_code))\n",
    "    exec('{}_author_abstract_funding_df.to_csv(\"scopus_data/{}_author_abstract_funding.csv\",index=False, encoding=\"utf-8\")'.format(pub_code, pub_code))\n",
    "\n",
    "    # exec('pub_dict[\"{}\"][\"{}_cites_df\"] = {}_cites_df'.format(pub_code,pub_code, pub_code))\n",
    "    # exec('{}_cites_df.to_csv(\"scopus_data/{}_cites.csv\",index=False, encoding=\"utf-8\")'.format(pub_code, pub_code))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Econometrica\n",
      "1684\n",
      "-------------------------------------------------\n",
      "Journal of Political Economy\n",
      "1331\n",
      "-------------------------------------------------\n",
      "Quarterly Journal of Economics\n",
      "1321\n",
      "-------------------------------------------------\n",
      "American Economic Review\n",
      "5139\n",
      "-------------------------------------------------\n",
      "Review of Economic Studies\n",
      "1462\n",
      "-------------------------------------------------\n",
      "RAND Journal of Economics\n",
      "1195\n",
      "-------------------------------------------------\n",
      "Journal of Finance\n",
      "2510\n",
      "-------------------------------------------------\n",
      "Journal of Financial Economics\n",
      "2873\n",
      "-------------------------------------------------\n",
      "Review of Financial Studies\n",
      "2078\n",
      "-------------------------------------------------\n",
      "Journal of Economics and Management Strategy\n",
      "746\n",
      "-------------------------------------------------\n",
      "Antitrust Law Journal\n",
      "440\n",
      "-------------------------------------------------\n",
      "Journal of Law and Economics\n",
      "729\n",
      "-------------------------------------------------\n",
      "Journal of Law, Economics, and Organization\n",
      "743\n",
      "-------------------------------------------------\n",
      "Journal of Labor Economics\n",
      "865\n",
      "-------------------------------------------------\n",
      "Journal of Human Resources\n",
      "980\n",
      "-------------------------------------------------\n",
      "Antitrust Bulletin\n",
      "770\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for pub in collection_dict.keys():\n",
    "    pub_dict[pub] = {}\n",
    "    pub_dict[pub]['{}_core_df'.format(pub)] = None\n",
    "    pub_dict[pub]['{}_author_abstract_df'.format(pub)] = None\n",
    "    pub_dict[pub]['{}_cites_df'.format(pub)] = None\n",
    "\n",
    "for pub in collection_dict.keys():\n",
    "    pub_code = pub\n",
    "    pub_issn = collection_dict.get(pub).get('issn')\n",
    "    pub_start_year = collection_dict.get(pub).get('start_year')\n",
    "    pub_name = collection_dict.get(pub).get('print_name')\n",
    "\n",
    "\n",
    "    print(pub_name)\n",
    "    pub_dict[pub_code]['{}_core_df'.format(pub_code)] = pd.read_csv('scopus_data/{}_scopus_core.csv'.format(pub_code))\n",
    "    print(len(pub_dict[pub_code]['{}_core_df'.format(pub_code)]))\n",
    "    print('-------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECX_core_df = pd.read_csv('scopus_data/ECA_scopus_core.csv').sample(n=100)\n",
    "\n",
    "# pub_dict = {\n",
    "#     'ECX' : {\n",
    "#         'ECX_core_df' : ECX_core_df,\n",
    "#         'ECX_author_author_abstract_df' : None,\n",
    "#         'ECX_cites_df' : None\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# pub_code='ECX'\n",
    "# print(ECX_core_df)\n",
    "# exec('{}_author_abstract_funding_df, {}_cites_df = abstract_references_collect(\"{}\")'.format(pub_code, pub_code, pub_code))\n",
    "# exec('pub_dict[\"{}\"][\"{}_author_abstract_df\"] = {}_author_abstract_funding_df'.format(pub_code,pub_code, pub_code))\n",
    "# # exec('{}_author_abstract_funding_df.to_csv(\"scopus_data/{}_author_abstract_funding.csv\",index=False, encoding=\"utf-8\")'.format(pub_code, pub_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a pub_code (see 'collection_dict' keys), this function identifies the publication-specific df generated by 'publication_collect', and generates two new dfs: 1) authors and abstract and funding; and 2) the citations that the article of interest makes. It then merges these back on to the pub_df (1:m) and returns that updated pub_df. We DO NOT merge on 'doi' (even though it should be a unique identifier) because if a SCOPUS object does not have an doi it the obersvation is coded as 'SCOPUS FAILURE' generically. This causes a (m:m) merge that is way too big/inaccurate. Instead we merge on 'sc_abstract_endpoint' which is a unique identifier generated by SCOPUS.\n",
    "def abstract_references_collect(pub_code):\n",
    "    pub_df = pub_dict.get(pub_code).get('{}_core_df'.format(pub_code))\n",
    "    abstract_query = {\n",
    "        'httpAccept' : 'application/json',\n",
    "        'view' : 'FULL'\n",
    "    }\n",
    "\n",
    "    articles = pub_df[['doi', 'sc_abstract_api_endpoint']]\n",
    "    articles.reset_index(inplace=True)\n",
    "    print(articles)\n",
    "\n",
    "    authors_abstracts_df = pd.DataFrame()\n",
    "    article_cites_df = pd.DataFrame()\n",
    "\n",
    "    # WE ARE GOING TO GO THROUGH EVERY ARTICLE IN THIS PUBLICATION \n",
    "    for row in range(0, len(articles)):\n",
    "        doi = articles.loc[row, 'doi']\n",
    "        url = articles.loc[row, 'sc_abstract_api_endpoint']\n",
    "        print('Working on: {}'.format(url))\n",
    "\n",
    "        # CLEAR TEMP DFS (These contain all the ____'s for a given article observation)\n",
    "        aa_df_temp = pd.DataFrame()\n",
    "        article_cites_df_temp = pd.DataFrame()\n",
    "    \n",
    "\n",
    "        abstract_r = requests.get(url, headers=req_headers, params=abstract_query)\n",
    "        if abstract_r.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        r_json_object = abstract_r.json()\n",
    "\n",
    "        author_affil_navigation_dict = {\n",
    "            'sc_author_id' : \"['author'][0]['@auid']\",\n",
    "            'sc_author_given_name' : \"['author'][0]['preferred-name']['ce:given-name']\",\n",
    "            'sc_author_last_name' : \"['author'][0]['preferred-name']['ce:surname']\",\n",
    "            'sc_author_indexed_name' : \"['author'][0]['preferred-name']['ce:indexed-name']\",\n",
    "            'sc_author_affil_id' : \"['affiliation']['affiliation-id']['@afid']\",\n",
    "            'sc_author_affil_indexed' : \"['affiliation']['ce:source-text']\",\n",
    "        }\n",
    "\n",
    "\n",
    "        aa_funding_navigation_dict = {\n",
    "            'sc_grant_text' : \"['abstracts-retrieval-response']['item']['bibrecord']['head']['grantlist']['grant-text']['$']\",\n",
    "            'sc_funding_text' : \"['abstracts-retrieval-response']['item']['xocs:meta']['xocs:funding-list']['xocs:funding-text']\",\n",
    "            'sc_funding_agency' : \"['abstracts-retrieval-response']['item']['xocs:meta']['xocs:funding']['xocs:funding-agency']\",\n",
    "        }\n",
    "\n",
    "\n",
    "        author_abstract_obs = pd.DataFrame({\n",
    "                'sc_author_id' : [None],\n",
    "                'sc_author_given_name' : [None],\n",
    "                'sc_author_last_name' : [None],\n",
    "                'sc_author_affil_id' : [None],\n",
    "                'sc_author_affil_indexed' : [None],\n",
    "                'sc_grant_text' :  [None],\n",
    "                'sc_funding_text' :  [None],\n",
    "                'sc_funding_agency' :  [None],\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ###################################\n",
    "        # WE DO AUTHORS AND ABSTRACTS FIRST\n",
    "        ###################################\n",
    "        try: \n",
    "            abstract_text = r_json_object['abstracts-retrieval-response']['item']['bibrecord']['head']['abstracts']\n",
    "            authors_object = r_json_object['abstracts-retrieval-response']['item']['bibrecord']['head']['author-group']\n",
    "           \n",
    "            aa_df_temp = author_obs_function(authors_object, aa_df_temp, author_abstract_obs, author_affil_navigation_dict)\n",
    "             \n",
    "            ### TRYING TO GET INFORMATION ON FUNDING IF ITS AVAILABLE\n",
    "            # EXPECT QUITE A FEW FAILURES/ 'SCOPUS FAILURES' IN THIS SECTION\n",
    "            aa_df_temp = funding_obs_function(aa_df_temp, r_json_object, aa_funding_navigation_dict)\n",
    "\n",
    "\n",
    "            aa_df_temp['sc_abstract_text'] = abstract_text\n",
    "            aa_df_temp['sc_abstract_api_endpoint'] = url\n",
    "\n",
    "\n",
    "        except:\n",
    "            print(\"FAILURE TO IDENTIFY ABSTRACT/AUTHORS FOR ARTICLE {}\\n\\t ENDPOINT QUERIED WAS: {}\".format(doi, url))\n",
    "\n",
    "            traceback = sys.exc_info()\n",
    "            failure_explanation = traceback[1]\n",
    "            failure_position = traceback[2].tb_lineno\n",
    "\n",
    "            # ABSTRACT IDENTIFICATION FAILURE\n",
    "            if failure_position == 65:\n",
    "                print('\\t FILLING IN FAILURE ON IDENTIFYING ABSTRACT IN JSON FOR ARTICLE: {}'.format(doi))\n",
    "                aa_df_temp = author_obs_function(authors_object, aa_df_temp, author_abstract_obs)\n",
    "                aa_df_temp = funding_obs_function(aa_df_temp, r_json_object, aa_funding_navigation_dict)\n",
    "\n",
    "                aa_df_temp['sc_abstract_api_endpoint'] = url\n",
    "\n",
    "            # AUTHOR IDENTIFICATION FAILURE\n",
    "            elif failure_position == 66:\n",
    "                print('\\t FILLING IN FAILURE ON IDENTIFYING AUTHORS IN JSON FOR ARTICLE: {}'.format(doi))\n",
    "                aa_df_temp = author_abstract_obs\n",
    "                aa_df_temp = aa_df_temp = funding_obs_function(aa_df_temp, r_json_object, aa_funding_navigation_dict)\n",
    "   \n",
    "                aa_df_temp['sc_abstract_api_endpoint'] = url  \n",
    "                aa_df_temp['sc_abstract_text'] = abstract_text       \n",
    "\n",
    "\n",
    "            print(\"\\tFAILED ON LINE {}\".format(failure_position))\n",
    "            print(\"\\tFAILED BECAUSE {}\".format(failure_explanation))\n",
    "\n",
    "\n",
    "        #################################\n",
    "        # WE DO REFERENCES AND CITES NEXT\n",
    "        #################################\n",
    "\n",
    "        try: \n",
    "            references_object = r_json_object['abstracts-retrieval-response']['item']['bibrecord']['tail']['bibliography']\n",
    "\n",
    "            article_cites_df_temp = references_function(references_object, doi, url)\n",
    "\n",
    "        except:\n",
    "\n",
    "            article_cites_df_temp = pd.DataFrame({\n",
    "                'doi' : [doi],\n",
    "                'sc_abstract_api_endpoint' : [url],\n",
    "                'sc_article_cites_scopus_citation_text' : [None],\n",
    "                'sc_article_cites_scopus_group_id' : [None],\n",
    "                'sc_article_cites_api_endpoint_list' : [None]\n",
    "            })\n",
    "\n",
    "            print(\"FAILURE TO IDENTIFY REFERENCES FOR ARTICLE {}\\n\\t ENDPOINT QUERIED WAS: {}\".format(doi, url))\n",
    "\n",
    "            traceback = sys.exc_info()\n",
    "            failure_explanation = traceback[1]\n",
    "            failure_position = traceback[2].tb_lineno\n",
    "\n",
    "        \n",
    "\n",
    "        # CONCATENATE THE ARTICLES AUTHORS/ABSTRACTS AND REFERENCES TO THE PUBLICATION-LEVEL DFS, RESPECTIVELY\n",
    "\n",
    "        authors_abstracts_df = pd.concat([authors_abstracts_df, aa_df_temp], ignore_index=True)\n",
    "        article_cites_df = pd.concat([article_cites_df, article_cites_df_temp], ignore_index=True)\n",
    "\n",
    "\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    \n",
    "    # MERGE THE AUTHORS/ABSTRACTS AND REFERENCES DATA ONTO THE THE CORE ARTICLE DATA (AT THE pub_df LEVEL)\n",
    "    \n",
    "    # author_abstract_funding_df = pd.merge(pub_df, authors_abstracts_df, how='left', on='doi')\n",
    "    # cites_df = pd.merge(pub_df, article_cites_df, how='left', on='doi')\n",
    "    \n",
    "    author_abstract_funding_df = pd.merge(pub_df, authors_abstracts_df, how='left', on='sc_abstract_api_endpoint')\n",
    "    cites_df = pd.merge(pub_df, article_cites_df, how='left', on='sc_abstract_api_endpoint')\n",
    "\n",
    "    \n",
    "\n",
    "    return author_abstract_funding_df, cites_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_obs_function(authors_json_object, df_so_far, author_abstract_obs_df, author_affil_navigation_dict):\n",
    "    # IF there is only a single author then this is a json/dict...\n",
    "    if type(authors_json_object) == dict:\n",
    "        print('authors_json_object that `author_obs_function()` received was a dictionary.')\n",
    "        aa_json = authors_json_object\n",
    "        author_abstract_obs = author_abstract_obs_df\n",
    "\n",
    "\n",
    "        if len(authors_json_object['author']) > 1:\n",
    "            print('do the different things')\n",
    "            for i, author_with_same_affil in enumerate(authors_json_object['author']):\n",
    "                author_affil_navigation_dict = {\n",
    "                            'sc_author_id' : \"['author'][0]['@auid']\",\n",
    "                            'sc_author_given_name' : \"['author'][0]['preferred-name']['ce:given-name']\",\n",
    "                            'sc_author_last_name' : \"['author'][0]['preferred-name']['ce:surname']\",\n",
    "                            'sc_author_indexed_name' : \"['author'][0]['preferred-name']['ce:indexed-name']\",\n",
    "                            'sc_author_affil_id' : \"['affiliation']['affiliation-id']['@afid']\",\n",
    "                            'sc_author_affil_indexed' : \"['affiliation']['ce:source-text']\",\n",
    "                        }\n",
    "                for field in author_affil_navigation_dict:\n",
    "                    author_abstract_obs = field_population(author_abstract_obs, aa_json, author_affil_navigation_dict, field, optional_dict_modification=i)\n",
    "            aa_df_temp = pd.concat([df_so_far, author_abstract_obs], ignore_index=True)\n",
    "            return aa_df_temp\n",
    "        else:\n",
    "            print('do the standard thing')\n",
    "                    \n",
    "            for field in author_affil_navigation_dict:\n",
    "                author_abstract_obs = field_population(author_abstract_obs, aa_json, author_affil_navigation_dict, field)\n",
    "\n",
    "            aa_df_temp = pd.concat([df_so_far, author_abstract_obs], ignore_index=True)\n",
    "            return aa_df_temp\n",
    "\n",
    "    #... but if there are multiple authors this is a list of jsons/dicts\n",
    "    else:\n",
    "        print('authors_json_object that `author_obs_function()` received was a list (of dictionaries?).')\n",
    "        aa_df_temp = df_so_far\n",
    "        for author_affil in authors_json_object:\n",
    "\n",
    "            author_abstract_obs = pd.DataFrame({\n",
    "                'sc_author_id' : [None],\n",
    "                'sc_author_given_name' : [None],\n",
    "                'sc_author_last_name' : [None],\n",
    "                'sc_author_affil_id' : [None],\n",
    "                'sc_author_affil_indexed' : [None],\n",
    "                'sc_grant_text' :  [None],\n",
    "                'sc_funding_text' :  [None],\n",
    "                'sc_funding_agency' :  [None],\n",
    "            })\n",
    "\n",
    "            aa_json = author_affil\n",
    "\n",
    "            if len(aa_json['author']) > 1:\n",
    "                print('do the different things')\n",
    "                for j, author_with_same_affil in enumerate(aa_json['author']):\n",
    "                    author_affil_navigation_dict = {\n",
    "                            'sc_author_id' : \"['author'][0]['@auid']\",\n",
    "                            'sc_author_given_name' : \"['author'][0]['preferred-name']['ce:given-name']\",\n",
    "                            'sc_author_last_name' : \"['author'][0]['preferred-name']['ce:surname']\",\n",
    "                            'sc_author_indexed_name' : \"['author'][0]['preferred-name']['ce:indexed-name']\",\n",
    "                            'sc_author_affil_id' : \"['affiliation']['affiliation-id']['@afid']\",\n",
    "                            'sc_author_affil_indexed' : \"['affiliation']['ce:source-text']\",\n",
    "                        }\n",
    "                    for field in author_affil_navigation_dict:\n",
    "                        author_abstract_obs = field_population(author_abstract_obs, aa_json, author_affil_navigation_dict, field, optional_dict_modification=j)\n",
    "\n",
    "                aa_df_temp = pd.concat([aa_df_temp, author_abstract_obs], ignore_index=True)\n",
    "                \n",
    "            else:\n",
    "                print('do the standard thing thing')\n",
    "                author_affil_navigation_dict = {\n",
    "                            'sc_author_id' : \"['author'][0]['@auid']\",\n",
    "                            'sc_author_given_name' : \"['author'][0]['preferred-name']['ce:given-name']\",\n",
    "                            'sc_author_last_name' : \"['author'][0]['preferred-name']['ce:surname']\",\n",
    "                            'sc_author_indexed_name' : \"['author'][0]['preferred-name']['ce:indexed-name']\",\n",
    "                            'sc_author_affil_id' : \"['affiliation']['affiliation-id']['@afid']\",\n",
    "                            'sc_author_affil_indexed' : \"['affiliation']['ce:source-text']\",\n",
    "                        }\n",
    "\n",
    "\n",
    "                for field in author_affil_navigation_dict:\n",
    "                    author_abstract_obs = field_population(author_abstract_obs, aa_json, author_affil_navigation_dict, field)\n",
    "\n",
    "                aa_df_temp = pd.concat([aa_df_temp, author_abstract_obs], ignore_index=True)\n",
    "\n",
    "        # There are some journals which have a strange formatting thing going on whereby a single author with multiple affiliations becomes multiple authors each with a single affiliation. What follows concatenates those \"sc_author_affil_id\" and \"sc_author_affil_indexed\"\n",
    "        \n",
    "        if True in aa_df_temp.duplicated(subset=['sc_author_id']).unique().tolist():\n",
    "            print('found a duplicated author (i.e with multiple affiliations)')\n",
    "            affil_id_concat = aa_df_temp.groupby('sc_author_id')['sc_author_affil_id'].apply(lambda x: '+'.join(x)).reset_index()\n",
    "            affil_index_concat = aa_df_temp.groupby('sc_author_id')['sc_author_affil_indexed'].apply(lambda x: '+'.join(x)).reset_index()\n",
    "\n",
    "            concatenated_obs = pd.concat([affil_id_concat, affil_index_concat['sc_author_affil_indexed']], axis=1)\n",
    "            with_concat_df = pd.merge(aa_df_temp, concatenated_obs, how='left', on='sc_author_id')\n",
    "\n",
    "            with_concat_df = with_concat_df.drop(['sc_author_affil_id_x', 'sc_author_affil_indexed_x'], axis=1)\n",
    "            with_concat_df.rename(columns={\n",
    "                'sc_author_affil_id_y' : 'sc_author_affil_id',\n",
    "                'sc_author_affil_indexed_y' : 'sc_author_affil_indexed',\n",
    "            }, inplace=True)\n",
    "            aa_df_temp = with_concat_df[~with_concat_df.duplicated(subset=['sc_author_id'], keep='first')]\n",
    "\n",
    "\n",
    "        return aa_df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funding_obs_function(df_so_far, json_object, aa_funding_navigation_dict):\n",
    "    aa_df_temp = df_so_far\n",
    "    for field in aa_funding_navigation_dict:\n",
    "        aa_df_temp = field_population(aa_df_temp, json_object, aa_funding_navigation_dict, field)\n",
    "\n",
    "    return aa_df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def references_function(references_object, doi, url):\n",
    "    references_list = references_object['reference']\n",
    "\n",
    "    sc_article_cites_scopus_group_id_list = []\n",
    "    sc_article_cites_scopus_citation_text_list = []\n",
    "    sc_article_cites_api_endpoint_list = []\n",
    "\n",
    "    for ref in references_list:\n",
    "        # The bibliography returns itemids of scopus-group id types rather than scopus id type. I don't think it should matter long-run because the articles can still be accessed by the 'https://api.elsevier.com/content/abstract/scopus_id/{scopus_id}' API. For more info see: https://silo.tips/download/sciverse-scopus-custom-data-documentation page 59.\n",
    "\n",
    "        try:\n",
    "            sc_article_cites_scopus_citation_text = ref['ref-fulltext']\n",
    "        except:\n",
    "            sc_article_cites_scopus_citation_text = \"SCOPUS FAILURE\"\n",
    "        try:\n",
    "            sc_article_cites_scopus_group_id = ref['ref-info']['refd-itemidlist']['itemid']['$']\n",
    "        except:\n",
    "            sc_article_cites_scopus_group_id = \"SCOPUS FAILURE\"\n",
    "\n",
    "        if sc_article_cites_scopus_group_id == \"SCOPUS FAILURE\":\n",
    "            sc_article_cites_api_endpoint = \"SCOPUS FAILURE\"\n",
    "        else:\n",
    "            sc_article_cites_api_endpoint = 'https://api.elsevier.com/content/abstract/scopus_id/{}'.format(sc_article_cites_scopus_group_id)\n",
    "\n",
    "\n",
    "        sc_article_cites_scopus_group_id_list.append(sc_article_cites_scopus_group_id)\n",
    "        sc_article_cites_scopus_citation_text_list.append(sc_article_cites_scopus_citation_text)\n",
    "        sc_article_cites_api_endpoint_list.append(sc_article_cites_api_endpoint)\n",
    "\n",
    "    article_cites_df_temp = pd.DataFrame({\n",
    "        'doi' : doi,\n",
    "        'sc_abstract_api_endpoint' : url,\n",
    "        'sc_article_cites_scopus_citation_text' : sc_article_cites_scopus_citation_text_list,\n",
    "        'sc_article_cites_scopus_group_id' : sc_article_cites_scopus_group_id_list,\n",
    "        'sc_article_cites_api_endpoint_list' : sc_article_cites_api_endpoint\n",
    "    })\n",
    "\n",
    "    return article_cites_df_temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## JOURNAL LEVEL CHECK API CALL\n",
    "\n",
    "human_query_issn = 'ISSN({issn_str})'.format(issn_str='00129682')\n",
    "human_query_date = '{start_year_str}-{current_year_str}'.format(start_year_str=1990,\n",
    "                                                                current_year_str=2021)\n",
    "\n",
    "\n",
    "url = 'https://api.elsevier.com/content/search/scopus'\n",
    "\n",
    "call_query = {\n",
    "                'httpAccept' : 'application/json',\n",
    "                'query' : human_query_issn,\n",
    "                'date' : human_query_date,\n",
    "                'count' : '25',\n",
    "                'cursor' : '*',\n",
    "                'view' : 'COMPLETE'\n",
    "            }\n",
    "\n",
    "# THIS IS WHERE WE CALL THE API TO GET THE LIST OF ARTICLES (x AT A TIME BY USING 'cursor/@next')\n",
    "test = requests.get(url,\n",
    "                headers=req_headers,\n",
    "                params=call_query\n",
    "                )\n",
    "\n",
    "test.json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARTICLE/ABSTRACT LEVEL CHECK API CALL\n",
    "abstract_url = 'https://api.elsevier.com/content/abstract/scopus_id/85118971968'\n",
    "\n",
    "abstract_test = requests.get(abstract_url,\n",
    "    headers = req_headers,\n",
    "    params = {\n",
    "        'httpAccept' : 'application/json',\n",
    "        'view' : 'FULL'\n",
    "    })\n",
    "\n",
    "x = abstract_test.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': 'Sun, 24 Jul 2022 03:15:10 GMT', 'Content-Type': 'application/json;charset=UTF-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'allow': 'GET', 'Content-Encoding': 'gzip', 'Last-Modified': 'Mon, 17 Jan 2022 14:40:09 GMT', 'Vary': 'Accept-Encoding, Origin, Access-Control-Request-Method, Access-Control-Request-Headers', 'X-ELS-APIKey': '744c08b8143a77bb752f8b818fd65171', 'X-ELS-ReqId': '01b04f69f696d73d', 'X-ELS-ResourceVersion': 'default', 'X-ELS-Status': 'OK', 'X-ELS-TransId': '56a26c5b840d1e8d', 'X-RateLimit-Limit': '10000', 'X-RateLimit-Remaining': '9754', 'X-RateLimit-Reset': '1658734922', 'CF-Cache-Status': 'DYNAMIC', 'Expect-CT': 'max-age=604800, report-uri=\"https://report-uri.cloudflare.com/cdn-cgi/beacon/expect-ct\"', 'Server': 'cloudflare', 'CF-RAY': '72f97d615ccd8723-ORD'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_test.headers"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a6fdbea366c7b44d686c4b3e88b365fb58ab8fc9919f1d3dc4ca0435b8b7768"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
