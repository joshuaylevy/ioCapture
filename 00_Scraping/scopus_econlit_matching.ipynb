{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from thefuzz import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input_csvs(pub_code):\n",
    "    print('READING IN {}'.format(pub_code))\n",
    "    scopus_core_path = 'scopus_data/' + pub_code + '_scopus_core.csv'\n",
    "    econlit_path = 'econlit_xml_csv/' + pub_code + '_econlit.csv'\n",
    "\n",
    "    scopus_df = pd.read_csv(scopus_core_path, encoding='utf-8')\n",
    "    econlit_df = pd.read_csv(econlit_path, encoding='utf-8')\n",
    "\n",
    "    print('{} ------ NUMBER OF SCOPUS OBSERVATIONS: {}'.format(pub_code, len(scopus_df)))\n",
    "    print('{} ----- NUMBER OF ECONLIT OBSERVATIONS: {}'.format(pub_code, len(econlit_df)))\n",
    "\n",
    "\n",
    "    return scopus_df, econlit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_article_patterns = {\n",
    "    'addresses' : r'President\\'s address', \n",
    "    'abstracts' : r'^Abstracts', \n",
    "    'announcements' : r'Editorial Announcement|Announcements(\\.|:)',\n",
    "    'backmatter' : r'Back Cover|Back matter|Backmatter',\n",
    "    'corrigendum': r'Corrigendum( to)?:?',\n",
    "    'comment' : r':( A)? Comment|Comments? on|Comment:| Comment$|(: )?Notes and comments(: )?',\n",
    "    'contents' : r'(Table of )?contents\\.',\n",
    "    'correction' : r':( A)? Correction|Correction to',\n",
    "    'editorial' : r'^Editorial$|: Editorial',\n",
    "    'discussion' : r':( Panel)? discussion|^Discussion\\.?$',\n",
    "    'errata': r'Errat(a|um):?|Errata( and corrections)?',\n",
    "    'forthcoming' : r'forthcoming papers',\n",
    "    'frontmatter' : r'Front Matter|Frontmatter|Preface|Masthead|^Introduction$|Foreword|Editors\\' introduction',\n",
    "    'indices' : r'Index(\\.)',\n",
    "    'manuscripts' : r'(accepted|forthcoming) manuscripts',\n",
    "    'minutes' : r'Minutes of the',\n",
    "    'notes' : r'Editors\\' notes?',\n",
    "    'referees': r'Referees\\.',\n",
    "    'rejoinder': r'(: )?Rejoinder:?|Rejoinder to',\n",
    "    'reply': r'Reply:|Response:|: Reply',\n",
    "    'response' : r':( A)? Response|A Response \\[',\n",
    "    'reporting' : r'Recent Referees\\.|Report of the (secretary|treasurer|editor|president|(search |finance )?committee|director|representative)|List of online reports',\n",
    "    'others' : r'Nobel lecture|\\(Book|Book review\\.|News notes\\.|Invited papers and discussions|Job openings for',\n",
    "    'aer_specific' : r'Papers and Proceedings of|:? Distinguished Fellow|Ad hoc Committee|American Economic Review| American Economic Journal:|American Economic Association|Journal of Economic (Perspectives|Literature)',\n",
    "    'eca_specific' : r'Econometric Society|Submission of Manuscripts to Econometrica|Nomination of fellows',\n",
    "    'res_specific' : r'Review of Economic Studies',\n",
    "    'jpe_specific' : r'JPE Turnaround Times(, Previous Two Years)?|Index to Vo',\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_non_articles(pub_code, scopus_df, econlit_df, print_mode=\"none\"):\n",
    "\n",
    "    scopus_df['non_article_indicator'] = scopus_df['sc_title'].apply(lambda x: pattern_matching(x))\n",
    "    econlit_df['non_article_indicator'] = econlit_df['title'].apply(lambda x: pattern_matching(x))\n",
    "\n",
    "    scopus_non_articles_df = scopus_df[scopus_df.non_article_indicator ==1]\n",
    "    scopus_non_articles_titles = len(scopus_non_articles_df)\n",
    "    econlit_non_articles_df = econlit_df[econlit_df.non_article_indicator ==1]\n",
    "    econlit_non_articles_titles = len(econlit_non_articles_df)\n",
    "\n",
    "\n",
    "    print('{} ------ SCOPUS NON-ARTICLES REMOVED: {}'.format(pub_code, scopus_non_articles_titles))\n",
    "    print('{} ----- ECONLIT NON-ARTICLES REMOVED: {}'.format(pub_code, econlit_non_articles_titles))\n",
    "\n",
    "\n",
    "    if print_mode == \"remaining\":\n",
    "        # DEBUGGING/EDITING: Print on all articles *except* those that are filtered out\n",
    "        for title in scopus_df[scopus_df.non_article_indicator ==0]['sc_title'].tolist():\n",
    "            print(title)\n",
    "        print('-------------------------------------------------------------------')\n",
    "        for title in econlit_df[econlit_df.non_article_indicator==0]['title'].tolist():\n",
    "            print(title)\n",
    "    elif print_mode == \"removed\":\n",
    "        # DEBUGGING/EDITING: Print *only* articles that are filtered out\n",
    "        for title in scopus_df[scopus_df.non_article_indicator == 1]['sc_title'].tolist():\n",
    "            print(title)\n",
    "        print('-------------------------------------------------------------------')\n",
    "        for title in econlit_df[econlit_df.non_article_indicator == 1]['title'].tolist():\n",
    "            print(title)\n",
    "        \n",
    "\n",
    "\n",
    "    scopus_df = scopus_df[scopus_df.non_article_indicator == 0]\n",
    "    econlit_df = econlit_df[econlit_df.non_article_indicator ==0]\n",
    "\n",
    "\n",
    "\n",
    "    return scopus_df, econlit_df\n",
    "\n",
    "def pattern_matching(title_string):\n",
    "    for pattern in non_article_patterns.values():\n",
    "        if re.search(pattern, title_string, re.I):\n",
    "            return 1\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_match(scopus_df, econlit_df):\n",
    "    scopus_df['sc_title_upper'] = scopus_df['sc_title'].apply(lambda x: x.upper())\n",
    "    econlit_df['title_upper'] = econlit_df['title'].apply(lambda x: x.upper())\n",
    "\n",
    "    try:\n",
    "        naive_match_df = pd.merge(scopus_df, econlit_df,\n",
    "            how='outer',\n",
    "            left_on=['sc_vol', 'sc_issue', 'sc_title_upper'],\n",
    "            right_on=['volume', 'issue', 'title_upper'],\n",
    "            indicator=True)\n",
    "\n",
    "    except ValueError:\n",
    "        print('Initial naive match failed.\\nTrying naive match again with coerced column-types')\n",
    "        scopus_df = scopus_df.astype({\n",
    "            'sc_title_upper' : 'object',\n",
    "            'sc_vol' : 'str',\n",
    "            'sc_issue' : 'str'\n",
    "        })\n",
    "\n",
    "        econlit_df = econlit_df.astype({\n",
    "            'title_upper' : 'object',\n",
    "            'volume' : 'str',\n",
    "            'issue' : 'str'\n",
    "        })\n",
    "\n",
    "        naive_match_df = pd.merge(scopus_df, econlit_df,\n",
    "            how='outer',\n",
    "            left_on=['sc_vol', 'sc_issue', 'sc_title_upper'], \n",
    "            right_on=['volume', 'issue', 'title_upper'],\n",
    "            indicator=True)\n",
    "        \n",
    "\n",
    "    ### If the initial merge returns absolutely nothing (probably because issues are incorrect, we try a more permissive naive match)\n",
    "\n",
    "\n",
    "    return naive_match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_right_onlys(naive_match_df):\n",
    "    nm_match_count = len(naive_match_df[naive_match_df._merge == 'both'])\n",
    "    nm_scopus_only = naive_match_df[naive_match_df._merge == 'left_only']\n",
    "    nm_econlit_only = naive_match_df[naive_match_df._merge == 'right_only']\n",
    "\n",
    "    nm_scopus_only.reset_index(inplace=True)\n",
    "    nm_econlit_only.reset_index(inplace=True)\n",
    "\n",
    "    nm_scopus_only = nm_scopus_only.drop(columns=['_merge', 'level_0'], axis=1)\n",
    "    nm_econlit_only = nm_econlit_only.drop(columns=['_merge', 'level_0'], axis=1)\n",
    "\n",
    "    print('Number of naively-matched observations: {}'.format(nm_match_count))\n",
    "    print('Number of SCOPUS-only observations: {}'.format(len(nm_scopus_only)))\n",
    "    print('Number of ECONLIT-only observations: {}'.format(len(nm_econlit_only)))\n",
    "\n",
    "    return nm_scopus_only, nm_econlit_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_score_compute(econlit_row, scopus_volume, scopus_issue, scopus_pagerange, scopus_doi, scopus_title_upper, scopus_date):\n",
    "    econlit_volume = econlit_row.volume\n",
    "    econlit_issue = econlit_row.issue\n",
    "    econlit_doi = econlit_row.doi_y\n",
    "    econlit_pagerange = econlit_row.pages \n",
    "    econlit_title_upper = econlit_row.title_upper\n",
    "    econlit_date = econlit_row.date\n",
    "\n",
    "    scopus_volume_type = type(scopus_volume)\n",
    "    scopus_issue_type = type(scopus_issue)\n",
    "    econlit_volume_type = type(econlit_volume)\n",
    "    econlit_issue_type = type(econlit_issue)\n",
    "    type_list = [scopus_volume_type, scopus_issue_type, econlit_volume_type, econlit_issue_type]\n",
    "\n",
    "    number_types_set = {np.float64, float, int}\n",
    "    \n",
    "    # If everything already is of a numerical type, skip the below because we can just coerce everything to an int() and efficiently check for matches that way\n",
    "    exception_status = 0\n",
    "    if not set(type_list).issubset(number_types_set):\n",
    "        for i, element in enumerate(type_list):\n",
    "            if element == str:\n",
    "                if i == 0:\n",
    "                    try:\n",
    "                        scopus_volume = int(scopus_volume)\n",
    "                        scopus_volume_type = int\n",
    "                    except:\n",
    "                        # print(\"COULD NOT COERCE SCOPUS VOLUME ({}) PROPERLY\".format(scopus_volume))\n",
    "                        exception_status += 1\n",
    "                        scopus_volume = -888888\n",
    "                elif i == 1:\n",
    "                    try:\n",
    "                        scopus_issue = int(scopus_issue)\n",
    "                        scopus_issue_type = int\n",
    "                    except:\n",
    "                        # print(\"COULD NOT COERCE SCOPUS ISSUE ({}) PROPERLY\".format(scopus_issue))\n",
    "                        exception_status += 1\n",
    "                        scopus_volume = -888888\n",
    "                elif i == 2:\n",
    "                    try:\n",
    "                        econlit_volume = int(econlit_volume)\n",
    "                        econlit_volume_type = int\n",
    "                    except:\n",
    "                        # print(\"COULD NOT COERCE ECONLIT VOLUME ({}) PROPERLY\".format(econlit_volume))\n",
    "                        exception_status += 1\n",
    "                        if econlit_volume == 'ECONLIT None Found':\n",
    "                            econlit_volume = -999999\n",
    "                            exception_status -= 1\n",
    "                elif i == 3:\n",
    "                    try:\n",
    "                        econlit_issue = int(econlit_issue)\n",
    "                        econlit_issue_type = int\n",
    "                    except:\n",
    "                        # print(\"COULD NOT COERCE ECONLIT ISSUE ({}) PROPERLY\".format(econlit_issue))\n",
    "                        exception_status += 1\n",
    "                        if econlit_issue == 'ECONLIT None Found':\n",
    "                            econlit_issue = -999999\n",
    "                            exception_status -= 1\n",
    "        # if exception_status == 1:\n",
    "        #     return 0\n",
    "\n",
    "\n",
    "\n",
    "    score = 0\n",
    "    ### 10 points for being in the correct volume, issue\n",
    "    if (int(scopus_volume), int(scopus_issue)) == (int(econlit_volume), int(econlit_issue)):\n",
    "        score += 10\n",
    "    ### 7 points for being in the at least the correct volume\n",
    "    elif int(scopus_volume) == int(econlit_volume):\n",
    "        score += 7\n",
    "    ### 2 points for being in at least the same issue number\n",
    "    elif (int(scopus_issue) == int(econlit_issue)) and int(scopus_issue) > 0:\n",
    "        score += 2\n",
    "    else:\n",
    "        score = 0\n",
    "        return score\n",
    "    ### 10 points for having *exactly* the same page range\n",
    "    if scopus_pagerange == econlit_pagerange:\n",
    "        score += 10\n",
    "\n",
    "    ### 7 points for the publication dates being *exactly* the same (This should be identical to the (Vol, iss) == (Vol, iss) condition)\n",
    "    if scopus_date == econlit_date:\n",
    "        score +=7\n",
    "\n",
    "\n",
    "\n",
    "    ### Up to 10 points for Title fuzzy-match edit-distance\n",
    "    set_edit_distance_ratio = fuzz.token_set_ratio(econlit_title_upper, scopus_title_upper)\n",
    "    sort_edit_distance_ratio = fuzz.token_sort_ratio(econlit_title_upper, scopus_title_upper)\n",
    "    gen_edit_distance_ratio = fuzz.ratio(econlit_title_upper, scopus_title_upper)\n",
    "    \n",
    "    # Each of the ratios returns a score on the interval [0,100] so the average will also be on this interval\n",
    "    average_edit_distance_ratio = np.mean([set_edit_distance_ratio, sort_edit_distance_ratio, gen_edit_distance_ratio])\n",
    "    \n",
    "    # print('\\t\\u251d{}\\n\\t|\\t\\t\\u251d-AVERAGE SCORE: {}\\n\\t|\\t\\t\\u251d-set score: {}\\n\\t|\\t\\t\\u251d-sort score: {}\\n\\t|\\t\\t\\u2517- gen score: {}'.format(econlit_title_upper, average_edit_distance_ratio, set_edit_distance_ratio, sort_edit_distance_ratio, gen_edit_distance_ratio))\n",
    "\n",
    "\n",
    "    # Divide the [0,100] by 10 to rescale to [0,10] points towards the custom score\n",
    "    score += average_edit_distance_ratio/10\n",
    "\n",
    "\n",
    "\n",
    "    # if exception_status >0 :\n",
    "    #     print('({},{}), ({},{})'.format(scopus_volume, scopus_issue, econlit_volume, econlit_issue))\n",
    "    #     return 0\n",
    "\n",
    "\n",
    "\n",
    "    return score\n",
    "\n",
    "def custom_scorer(scopus_row, econlit_only_df):\n",
    "    scopus_volume = scopus_row.sc_vol\n",
    "    scopus_issue = scopus_row.sc_issue\n",
    "    scopus_pagerange = scopus_row.sc_page_range\n",
    "    scopus_doi = scopus_row.doi_x\n",
    "    scopus_title_upper = scopus_row.sc_title_upper\n",
    "    scopus_date = scopus_row.sc_pub_date\n",
    "\n",
    "\n",
    "    econlit_only_df.loc[:, 'custom_match_score'] = econlit_only_df.apply(lambda x: custom_score_compute(x, scopus_volume, scopus_issue, scopus_pagerange, scopus_doi, scopus_title_upper, scopus_date), axis=1)\n",
    "\n",
    "    econlit_only_df_scored_list = econlit_only_df['custom_match_score'].tolist()\n",
    "    return econlit_only_df_scored_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_matching(score_matrix):\n",
    "    rows = score_matrix.shape[0]\n",
    "    cols = score_matrix.shape[1]\n",
    "    print(\"SCORE MATRIX DIMENSIONS: ({} ROWS, {} COLS)\".format(rows, cols))\n",
    "\n",
    "    matched_pairs = []\n",
    "\n",
    "    # rows\n",
    "    for scopus_index in range(0, rows):\n",
    "        # First we find the column (econlit article) that has the highest matching-score for this scopus observation\n",
    "        best_match_score = max(score_matrix[scopus_index])      \n",
    "        best_match_index = np.argmax(score_matrix[scopus_index])\n",
    "\n",
    "\n",
    "        # Then we make sure that the best match for this scopus article does not match better with another scopus article\n",
    "\n",
    "        best_matchs_match_index = np.argmax(score_matrix[:, best_match_index], axis=0)\n",
    "        best_matchs_match_score = score_matrix[best_matchs_match_index][best_match_index]\n",
    "\n",
    "    \n",
    "        if scopus_index == best_matchs_match_index:\n",
    "            ##### THIS IS THE KEY VALUE THAT WE USE TO DETERMINE THE THRESHOLD FOR WHETHER A FUZZY MATCH IS SUFFICIENTLY GOOD\n",
    "            if best_match_score < 19.9:\n",
    "                continue\n",
    "            else:\n",
    "                matched_pairs.append((int(scopus_index), int(best_match_index)))\n",
    "\n",
    "        else:\n",
    "            print('On Scopus index {}, the best match appears to be Econlit index {} (match score: {}), but that Econlit matches best with Scopus index {} (match score: {})'.format(scopus_index, best_match_index, best_match_score, best_matchs_match_index, best_matchs_match_score))\n",
    "\n",
    "    return matched_pairs\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_matches(matched_pairs, nm_scopus_only, nm_econlit_only):\n",
    "    print('Interpreting {} matches'.format(len(matched_pairs)))\n",
    "\n",
    "    for pair in matched_pairs:\n",
    "        scopus_index = pair[0]\n",
    "        econlit_index = pair[1]\n",
    "\n",
    "        scopus_title = nm_scopus_only.loc[scopus_index, 'sc_title_upper']\n",
    "        econlit_title = nm_econlit_only.loc[econlit_index, 'title_upper']\n",
    "\n",
    "        match_score = unmatched_score_matrix[scopus_index][econlit_index]\n",
    "\n",
    "        print('Based on scores({}), want to match (({})) with (({}))'.format(match_score, scopus_title, econlit_title))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_instantiation(scopus_only, econlit_only):\n",
    "    # Rows (number of scopus-only articles)\n",
    "    n = len(scopus_only)\n",
    "    # Columns (number of econlit-only articles)\n",
    "    m = len(econlit_only)\n",
    "\n",
    "    matrix = np.zeros((n,m))\n",
    "\n",
    "    for i in range(0,n):\n",
    "        scopus_row = scopus_only.iloc[i, :]\n",
    "        temp_econlit_df = econlit_only\n",
    "        print(scopus_row.sc_title_upper)\n",
    "        matrix[i][0:m] = custom_scorer(scopus_row, temp_econlit_df)\n",
    "        print('\\n\\n')\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_fuzzy_matches(fuzzy_matches_indices, scopus_only, econlit_only, naive_matched_df):\n",
    "    \n",
    "    scopus_columns = ['doi_x', 'sc_title', 'sc_issn', 'sc_pub_name', 'sc_vol', 'sc_issue', 'sc_page_range', 'sc_abstract_api_endpoint', 'sc_human_url', 'sc_pub_date', 'sc_open_access_status', 'sc_query_used', 'sc_title_upper']\n",
    "    econlit_columns = ['jel_desc', 'jel_code', 'doi_y', 'title', 'volume', 'issue', 'date', 'pages', 'issn', 'author', 'abstract', 'title_upper']\n",
    "    \n",
    "    matching_columns = scopus_columns + econlit_columns\n",
    "\n",
    "    \n",
    "    fuzzy_matched_df = pd.DataFrame(np.full((0, len(matching_columns)), np.nan), columns=matching_columns)\n",
    "\n",
    "    for i, pair in enumerate(fuzzy_matches_indices):\n",
    "        scopus_index = pair[0]\n",
    "        econlit_index = pair[1]\n",
    "\n",
    "        fuzzy_matched_df.loc[i, scopus_columns] = scopus_only.loc[scopus_index, scopus_columns]\n",
    "        fuzzy_matched_df.loc[i, econlit_columns] = econlit_only.loc[econlit_index, econlit_columns]\n",
    "\n",
    "\n",
    "    fuzzy_matches_appended = pd.concat([naive_matched_df, fuzzy_matched_df], ignore_index=True)\n",
    "    return fuzzy_matches_appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_unmatched_remainders(matched_indices, nm_scopus_only_df, nm_econlit_only_df):\n",
    "    \n",
    "    fuzzy_unmatched_scopus = nm_scopus_only_df\n",
    "    fuzzy_unmatched_econlit = nm_econlit_only_df\n",
    "\n",
    "    for pair in matched_indices:\n",
    "        matched_scopus_index = pair[0]\n",
    "        matched_econlit_index = pair[1]\n",
    "\n",
    "        fuzzy_unmatched_scopus = fuzzy_unmatched_scopus.drop([matched_scopus_index])\n",
    "        fuzzy_unmatched_econlit = fuzzy_unmatched_econlit.drop([matched_econlit_index])\n",
    "\n",
    "\n",
    "    return fuzzy_unmatched_scopus, fuzzy_unmatched_econlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_fuzzy_matched_etc(pub_code: str, fuzzy_matches_appended: pd.DataFrame, fuzzy_unmatched_scopus: pd.DataFrame, fuzzy_unmatched_econlit: pd.DataFrame):\n",
    "    fuzzy_folder_path = 'econlit_scopus_matching_out/{}_fuzzy_results/'.format(pub_code)\n",
    "    fuzzy_matches_appended_path = fuzzy_folder_path + '{}_fuzzy_matches.csv'.format(pub_code)\n",
    "    unmatched_scopus_path = fuzzy_folder_path + '{}_fuzzy_unmatched_scopus.csv'.format(pub_code)\n",
    "    unmatched_econlit_path = fuzzy_folder_path + '{}_fuzzy_unmatched_econlit.csv'.format(pub_code)\n",
    "\n",
    "    if os.path.exists(fuzzy_folder_path):\n",
    "        fuzzy_matches_appended.to_csv(fuzzy_matches_appended_path, encoding='utf-8', index=False)\n",
    "        fuzzy_unmatched_scopus.to_csv(unmatched_scopus_path, encoding='utf-8', index=False)\n",
    "        fuzzy_unmatched_econlit.to_csv(unmatched_econlit_path, encoding='utf-8', index=False)\n",
    "\n",
    "\n",
    "    else:\n",
    "        print('{} folder/path does not exist'.format(fuzzy_folder_path))\n",
    "        print('Creating path now')\n",
    "        os.makedirs(fuzzy_folder_path)\n",
    "        output_fuzzy_matched_etc(pub_code, fuzzy_matches_appended, fuzzy_unmatched_scopus, fuzzy_unmatched_econlit)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_matching_report(pub_code, scopus_df, econlit_df, naive_match_df, nm_scopus_only, nm_econlit_only, fuzzy_matches_appended, fuzzy_unmatched_scopus, fuzzy_unmatched_econlit):\n",
    "\n",
    "    bars = '----------------------------------------------------------------------'\n",
    "\n",
    "    # ORIGINALS\n",
    "    scopus_original = 'Original {} Scopus-collected observations: {}'.format(pub_code, len(scopus_df))\n",
    "    econlit_original = 'Original {} EconLit-collected observations: {}'.format(pub_code, len(econlit_df))\n",
    "    total_original = 'Original {} collection TOTAL observations: {}'.format(pub_code, len(scopus_df) + len(econlit_df))\n",
    "\n",
    "\n",
    "    # POST-NAIVE-MATCH\n",
    "    naive_match = 'Number of {} naively-matched observations: {}'.format(pub_code, len(naive_match_df))\n",
    "    post_naive_scopus_only = 'Post-naive match {} Scopus-only observations: {}'.format(pub_code, len(nm_scopus_only))\n",
    "    post_naive_econlit_only = 'Post-naive match {} EconLit-only observations: {}'.format(pub_code, len(nm_econlit_only))\n",
    "\n",
    "\n",
    "    ### POST-FUZZY-MATCH\n",
    "    fuzzy_matches_added = len(fuzzy_matches_appended) - len(naive_match_df)\n",
    "    fuzzy_match = 'Number of {} fuzzy-matched observations {} ({} added)'.format(pub_code, len(fuzzy_matches_appended), fuzzy_matches_added)\n",
    "    post_fuzzy_scopus_only = 'Number of {} post-fuzzy-match Scopus-only remainder observations: {}'.format(pub_code, len(fuzzy_unmatched_scopus))\n",
    "    post_fuzzy_econlit_only = 'Number of {} post-fuzzy-match EconLit-only remainder observations: {}'.format(pub_code, len(fuzzy_unmatched_econlit))\n",
    "    total_fuzzy_unmatched = len(fuzzy_unmatched_scopus) + len(fuzzy_unmatched_econlit)\n",
    "\n",
    "    fuzzy_unmatched_summary = '{} observations ({}% of {}) remain unmatched.'.format(total_fuzzy_unmatched, round((total_fuzzy_unmatched * 100 )/ len(fuzzy_matches_appended), 2), len(fuzzy_matches_appended))\n",
    "\n",
    "\n",
    "    try: \n",
    "        fuzzy_unmatched_scopus['titles_plus_plus'] = fuzzy_unmatched_scopus.apply(lambda obs: '{} (VOL. {} ({}))'.format(obs.sc_title, int(obs.sc_vol), int(obs.sc_issue)), axis=1)\n",
    "    except:\n",
    "        fuzzy_unmatched_scopus['titles_plus_plus'] = fuzzy_unmatched_scopus.apply(lambda obs: '{} (VOL. {} ({}))'.format(obs.sc_title, obs.sc_vol, obs.sc_issue), axis=1)\n",
    "    try: \n",
    "        fuzzy_unmatched_econlit['titles_plus_plus'] = fuzzy_unmatched_econlit.apply(lambda obs: '{} (VOL. {} ({}))'.format(obs.title, int(obs.volume), int(obs.issue)), axis=1)\n",
    "    except:\n",
    "        fuzzy_unmatched_econlit['titles_plus_plus'] = fuzzy_unmatched_econlit.apply(lambda obs: '{} (VOL. {} ({}))'.format(obs.title, obs.volume, obs.issue), axis=1)\n",
    "\n",
    "    fuzzy_unmatched_scopus.sort_values(by=['sc_vol', 'sc_issue', 'sc_title'], inplace=True)\n",
    "    fuzzy_unmatched_econlit.sort_values(by=['volume', 'issue', 'title'], inplace=True)\n",
    "\n",
    "    post_fuzzy_scopus_titles = fuzzy_unmatched_scopus.titles_plus_plus.tolist()\n",
    "    post_fuzzy_econlit_titles = fuzzy_unmatched_econlit.titles_plus_plus.tolist()\n",
    "\n",
    "    \n",
    "    lines = ['FUZZY MATCHING REPORT', bars, scopus_original, econlit_original, total_original, bars, naive_match, post_naive_scopus_only, post_naive_econlit_only, bars, fuzzy_match, post_fuzzy_scopus_only, post_fuzzy_econlit_only, fuzzy_unmatched_summary, bars]\n",
    "\n",
    "    matching_report_path = 'econlit_scopus_matching_out/{}_fuzzy_results/{}_fuzzy_matching_report.txt'.format(pub_code, pub_code)\n",
    "    with open(matching_report_path, 'w', encoding='utf-8') as report:\n",
    "        report.write(str(datetime.now()))\n",
    "        for line in lines:\n",
    "            report.write('\\n{}'.format(line))\n",
    "\n",
    "        report.write('\\nSCOPUS ARTICLES REMAINING UNMATCHED ({})\\n'.format(len(fuzzy_unmatched_scopus)))\n",
    "        for title in post_fuzzy_scopus_titles:\n",
    "            report.write('\\n\\t{}'.format(title))\n",
    "\n",
    "\n",
    "\n",
    "        report.write('\\nECONLIT ARTICLES REMAINING UNMATCHED ({})\\n'.format(len(fuzzy_unmatched_econlit)))\n",
    "        for title in post_fuzzy_econlit_titles:\n",
    "            report.write('\\n\\t{}'.format(title))\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_list = [\n",
    "    # 'AER',\n",
    "    # 'ECA',\n",
    "    # 'JPE',\n",
    "    # 'QJE',\n",
    "    'RES',\n",
    "    # 'RJE',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pub_code in run_list:\n",
    "    scopus_df, econlit_df = load_input_csvs(pub_code)\n",
    "    print('----------------------------------------')\n",
    "    scopus_df, econlit_df = filter_non_articles(pub_code, scopus_df, econlit_df, \"remaining\")\n",
    "    naive_match_df = naive_match(scopus_df, econlit_df)\n",
    "    print('----------------------------------------')\n",
    "    nm_scopus_only, nm_econlit_only = left_right_onlys(naive_match_df)\n",
    "    naive_match_df = naive_match_df[naive_match_df._merge == 'both']\n",
    "    naive_match_df.reset_index(inplace=True)\n",
    "    naive_match_df = naive_match_df.drop(columns=['_merge', 'level_0'], axis=1)\n",
    "    print('----------------------------------------')\n",
    "    unmatched_score_matrix = matrix_instantiation(nm_scopus_only, nm_econlit_only)\n",
    "    print(unmatched_score_matrix.shape)\n",
    "    matched_indices_list = index_matching(unmatched_score_matrix)\n",
    "    interpret_matches(matched_indices_list, nm_scopus_only, nm_econlit_only)\n",
    "    print('----------------------------------------')\n",
    "    fuzzy_matches_appended = append_fuzzy_matches(matched_indices_list, nm_scopus_only, nm_econlit_only, naive_match_df)\n",
    "    fuzzy_unmatched_scopus, fuzzy_unmatched_econlit = fuzzy_unmatched_remainders(matched_indices_list, nm_scopus_only, nm_econlit_only)\n",
    "    print('----------------------------------------')\n",
    "    output_fuzzy_matched_etc(pub_code, fuzzy_matches_appended, fuzzy_unmatched_scopus, fuzzy_unmatched_econlit)\n",
    "    generate_matching_report(pub_code, scopus_df, econlit_df, naive_match_df, nm_scopus_only, nm_econlit_only, fuzzy_matches_appended, fuzzy_unmatched_scopus, fuzzy_unmatched_econlit)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "301a221a682cbebc60020c6c7a0e12fb9a472db6fd70ca33686bd8433de05f70"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ioCapture')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
