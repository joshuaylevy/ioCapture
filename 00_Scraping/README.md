# README

## About
This folder is dedicated to data collection, cleaning, and matching.

## Files and Folders

### Folders
- econ_lit_xml
    - This folder contains the .xml files that are generated by the EconLit batch citation tool. After an EconLit query is generated, the batch citation tool asks you to submit an email address. Some minutes later an email is sent to that address which provides a link (live for 24 hours after sending) that allows you to download this .xml. 
    - Submit these queries and download these downloads at the journal-level.
    - The format for the .xml filename should be "[PUBCODE]_[Start YY]-[End YY].xml"
    - These are raw data files; they should **NOT** be edited.
- econlit_scopus_matching_out
    - This folder contains the .csv files that underlie the papers dataset(s). There are 4 types of .csv files:
    1. '[PUBCODE]_author_abstract_funding.csv'
        - This .csv contains article-author-level observations from the [Scopus Abstract Retrieval API](https://dev.elsevier.com/documentation/AbstractRetrievalAPI.wadl). Each article-level observation contains doi, volume, issue, date, publication name, title, page-range,  
        - This .csv file is generated by `scopus_read.ipynb` in the `abstract_references_collect()` function.
        - These are effectively raw data files; they should **NOT** be edited. 
    2. '[PUBCODE]_cites.csv'
    3. '[PUBCODE]_scopus_core.csv'
    4. '[pubcode]_econlit.csv' 
        - This .csv file contains article-level observations from EconLit. It is generated by `jel_scopus_matching.ipynb` which reads in the .xml files in the folder "00_Scraping/econ_lit_xml/". The article-level observations contains volume, issue, date, author, abstract, JEL code/description, title and some other extraneous data.
- nber_working_papers

### Scripts


### Misc.
- `env_keys.json`
    - This contains API keys for SCOPUS. VERY IMPORTANT TO NOT UPLOAD THIS TO GITHUB. ENSURE THAT IT IS IN YOUR `.gitignore` FILE.
- `proxies.txt`
    - John is subscribed to a proxy-hosting service. We use these proxies to route our traffic when web-scraping so that our IPs are masked. Note that the proxy addresses in this .txt file are scrambled. To actually use them, split each line on colons (':') and then rearrange in order [2,3,0,1]. VERY IMPORTANT TO NOT UPLOAD THIS TO GITHUB. ENSURE THAT IT IS IN YOUR `.gitignore` FILE.
- `test.json`
    - There are a bunch of API calls that get made throughout many of these files. I usually format the response as JSON objects. I often copy-paste those objects into a separate .json file so that there is nice formatting when I am writing code/figuring out how to extract the actual data of interest. This is that .json file 

